{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a0db12",
   "metadata": {},
   "source": [
    "## AI Chatbot Description\n",
    "\n",
    "This chatbot is a multipurpose conversational agent designed to handle various tasks and provide assistance across different domains. Here are some key characteristics defining this AI chatbot:\n",
    "\n",
    "### Features:\n",
    "\n",
    "- **Multi-Language Support:** The chatbot can communicate in multiple languages, allowing users to interact in their preferred language.\n",
    "\n",
    "- **Translation Capabilities:** It can translate text between different languages, facilitating seamless communication between users speaking different languages.\n",
    "\n",
    "- **Information Retrieval:** The chatbot can retrieve diverse types of information from external sources, including news headlines, weather updates, and Wikipedia summaries.\n",
    "\n",
    "- **Text Analysis:** It performs basic text analysis tasks such as extracting named entities and identifying date patterns related to living or deceased individuals.\n",
    "\n",
    "- **Engagement Features:** The chatbot includes features like fetching random jokes to keep interactions entertaining and engaging for users.\n",
    "\n",
    "- **User Interface:** It provides a user-friendly interface for interacting with users and displaying information in a structured and readable format.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Overall, this chatbot is designed to be versatile and useful for a wide range of purposes, from providing information and assistance to engaging users in conversation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75d465",
   "metadata": {},
   "source": [
    "# Chatbot Implementation Details\n",
    "\n",
    "## Overview\n",
    "This document provides an overview of the implementation details for the AI chatbot application.\n",
    "\n",
    "## Technologies Used\n",
    "- **Programming Language:** Python\n",
    "- **Machine Learning Framework:** TensorFlow\n",
    "- **Natural Language Processing (NLP) Library:** spaCy\n",
    "- **User Interface Library:** Tkinter\n",
    "\n",
    "## Key Components\n",
    "### 1. Input Processing\n",
    "- User input is received through a text input field in the Tkinter GUI.\n",
    "\n",
    "### 2. Language Handling\n",
    "- The chatbot identifies the language of the user input using spaCy's language detection capabilities.\n",
    "\n",
    "### 3. Translation\n",
    "- User input is translated into English using the Google Translate API for processing.\n",
    "\n",
    "### 4. Task Identification\n",
    "- Natural Language Understanding (NLU) techniques are employed to identify the user's intent or task.\n",
    "\n",
    "### 5. Task Execution\n",
    "- Based on the identified task, the chatbot executes the appropriate function or queries external APIs for information.\n",
    "\n",
    "### 6. Information Retrieval\n",
    "- Various types of information, such as news headlines, weather updates, or Wikipedia summaries, are retrieved from external sources via API requests.\n",
    "\n",
    "### 7. Text Analysis\n",
    "- Named Entity Recognition (NER) and sentiment analysis are performed using spaCy to extract relevant information from the user input.\n",
    "\n",
    "### 8. Response Generation\n",
    "- The chatbot generates a response based on the processed input and retrieved information.\n",
    "\n",
    "### 9. Output Presentation\n",
    "- The response is displayed in the Tkinter GUI for the user to view.\n",
    "\n",
    "## Future Enhancements\n",
    "- Integration with additional APIs for richer information retrieval.\n",
    "- Implementation of a more advanced dialog management system for better conversational flow.\n",
    "- Support for multi-language input and output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f159f",
   "metadata": {},
   "source": [
    "# Description of Imports\n",
    "\n",
    "In this section of code, we import various Python modules and libraries required for building and training a neural network for natural language processing (NLP).\n",
    "\n",
    "- **json**: This module is used for working with JSON data format. It can be used to load or save data in JSON format.\n",
    "\n",
    "- **numpy (as np)**: NumPy is a fundamental package for numerical computing in Python. We use it for working with matrices and data arrays.\n",
    "\n",
    "- **tensorflow (as tf)**: TensorFlow is a popular open-source library for machine learning and deep learning developed by Google. Here, we use TensorFlow for building the neural network.\n",
    "\n",
    "- **tensorflow.keras.models.Sequential**: This is the API for building sequential models in TensorFlow. We use it to define the architecture of our neural network.\n",
    "\n",
    "- **tensorflow.keras.layers**: Here, we import various layers that will be used in our neural network, such as fully connected layers, embedding layers, and global average pooling layers.\n",
    "\n",
    "- **tensorflow.keras.preprocessing.text.Tokenizer**: The Tokenizer is used for tokenizing text, i.e., converting text into a sequence of tokens or indices.\n",
    "\n",
    "- **tensorflow.keras.preprocessing.sequence.pad_sequences**: This module is used for padding sequences, which is often required when working with sequential data in neural networks.\n",
    "\n",
    "- **sklearn.preprocessing.LabelEncoder**: The LabelEncoder is used for encoding categorical labels into a numerical format.\n",
    "\n",
    "By combining these modules and libraries, we can build and train a neural network for natural language processing (NLP) using TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86581be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41ec8f",
   "metadata": {},
   "source": [
    "To begin, the provided Python code snippet opens a JSON file named `chatbot_data.json` and loads its contents into a Python dictionary named `data`.\n",
    "\n",
    "Subsequently, it initializes several lists to store data extracted from the JSON file:\n",
    "- `training_sentences`: This list will contain sentences (patterns) used for training the chatbot.\n",
    "- `training_labels`: It will store the corresponding labels (intents) for each training sentence.\n",
    "- `labels`: This list will hold unique intent labels extracted from the data.\n",
    "- `responses`: It will store the responses associated with each intent.\n",
    "\n",
    "The purpose of each list is as follows:\n",
    "- `training_sentences`: Used to store patterns or sentences that will be used as input during the training process.\n",
    "- `training_labels`: Holds the corresponding intent labels for the training sentences.\n",
    "- `labels`: Contains unique intent labels found in the dataset.\n",
    "- `responses`: Stores responses associated with each intent, which will be used to generate chatbot replies during interactions.\n",
    "\n",
    "This initialization process sets the stage for further data processing and model training steps in developing a functional chatbot system.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdb1859",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chatbot_data.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591960e4",
   "metadata": {},
   "source": [
    "The provided Python code snippet processes data stored in a JSON format to prepare it for training a chatbot model. Here's an overview of how it works:\n",
    "\n",
    "1. **Loading Data:**\n",
    "   - The code begins by opening a JSON file named `chatbot_data.json` and loading its contents into a Python dictionary named `data`.\n",
    "\n",
    "2. **Preparing Training Data:**\n",
    "   - Two empty lists, `training_sentences` and `training_labels`, are initialized to store training data.\n",
    "   - Another empty list, `responses`, is initialized to store responses associated with each intent.\n",
    "   - The code iterates over each intent in the `data['content']` section.\n",
    "   - For each intent, it iterates over the patterns (training sentences) within that intent.\n",
    "   - It appends each pattern to the `training_sentences` list and its corresponding tag (intent label) to the `training_labels` list.\n",
    "   - Additionally, it appends the responses associated with the intent to the `responses` list.\n",
    "   - If the intent's tag is not already in the `labels` list, it adds the tag to the `labels` list.\n",
    "\n",
    "3. **Calculating Number of Classes:**\n",
    "   - After processing all intents, the code calculates the number of unique classes (intents) by taking the length of the `labels` list.\n",
    "   - The result is stored in the variable `num_classes`, which represents the number of distinct intents in the dataset.\n",
    "\n",
    "This code snippet is typically part of the data preprocessing stage in training a chatbot model. It organizes the training data into sentences and their corresponding labels, making it suitable for use in machine learning algorithms such as natural language processing (NLP) models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "545f2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in data['content']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "    \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "\n",
    "        num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4d96a",
   "metadata": {},
   "source": [
    "In the provided code snippet, a `LabelEncoder` object named `lbl_encoder` is instantiated and used to transform the `training_labels` list.\n",
    "\n",
    "Here's what each step does:\n",
    "\n",
    "1. **Instantiation of LabelEncoder**: \n",
    "   - A `LabelEncoder` object is created to encode the categorical labels (intent tags) into numerical values.\n",
    "\n",
    "2. **Fitting the Label Encoder**:\n",
    "   - The `fit` method of the `LabelEncoder` object is called with the `training_labels` list as input.\n",
    "   - This step fits the label encoder to the unique labels present in the `training_labels` list, enabling it to learn the mapping between labels and numerical values.\n",
    "\n",
    "3. **Transforming Training Labels**:\n",
    "   - The `transform` method of the `LabelEncoder` object is applied to the `training_labels` list.\n",
    "   - This step transforms the categorical intent labels into numerical representations based on the mapping learned during the fitting stage.\n",
    "\n",
    "The purpose of this transformation is to prepare the categorical labels for consumption by machine learning algorithms, which typically require numerical inputs. By encoding the labels into numerical format, the data becomes compatible with various machine learning models for further processing and training.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28754618",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoder.fit(training_labels)\n",
    "training_labels = lbl_encoder.transform(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a78d8b",
   "metadata": {},
   "source": [
    "In the provided code snippet, a `Tokenizer` object is instantiated and used to tokenize the training sentences, converting them into sequences of integers. Here's what each step does:\n",
    "\n",
    "1. **Tokenizer Instantiation**:\n",
    "   - A `Tokenizer` object is created with specified parameters:\n",
    "     - `num_words`: The maximum number of words to keep, based on word frequency. Only the most common `vocab_size` words will be kept.\n",
    "     - `oov_token`: Out-of-vocabulary token to represent words that are not present in the tokenizer's word index.\n",
    "   \n",
    "2. **Fitting on Texts**:\n",
    "   - The `fit_on_texts` method of the `Tokenizer` object is called with the `training_sentences` list as input.\n",
    "   - This step updates the internal vocabulary based on the words present in the training sentences and assigns a unique index to each word.\n",
    "\n",
    "3. **Word Index**:\n",
    "   - The `word_index` attribute of the tokenizer is accessed to retrieve the word-to-index mapping learned during the fitting stage.\n",
    "\n",
    "4. **Texts to Sequences**:\n",
    "   - The `texts_to_sequences` method of the `Tokenizer` object is applied to the `training_sentences` list.\n",
    "   - This step converts each sentence in `training_sentences` into a sequence of integers based on the learned word index.\n",
    "\n",
    "5. **Padding Sequences**:\n",
    "   - The `pad_sequences` function is used to ensure that all sequences have the same length (`max_len`), either by truncating or padding them with zeros.\n",
    "   - `truncating='post'` indicates that sequences longer than `max_len` will be truncated from the end.\n",
    "   - `maxlen=max_len` specifies the maximum length of sequences after padding or truncating.\n",
    "\n",
    "The tokenization and padding processes are essential for preparing text data for input into machine learning models, ensuring consistent input dimensions regardless of the original text lengths. This standardized format allows the data to be efficiently processed by neural network architectures.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296aebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_len = 20\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_token)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded_sequences = pad_sequences(sequences, truncating='post', maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9aa551",
   "metadata": {},
   "source": [
    "The provided code snippet demonstrates the construction and training of a neural network model using Keras (with TensorFlow backend). Here's an explanation of each step:\n",
    "\n",
    "1. **Model Architecture**:\n",
    "   - A `Sequential` model is instantiated, representing a linear stack of layers.\n",
    "   - The first layer added to the model is an `Embedding` layer, which maps integer indices to dense vectors of fixed size (`embedding_dim`). This layer converts the input sequences of word indices into dense vectors.\n",
    "   - `GlobalAveragePooling1D` layer is added to compute the average of the embeddings across all words in the document. This layer reduces the sequence of embeddings into a single vector.\n",
    "   - Two fully connected (`Dense`) layers with ReLU activation functions (`'relu'`) are added to introduce non-linearity and increase model complexity.\n",
    "   - The final `Dense` layer has `num_classes` neurons and uses a softmax activation function to output probabilities for each class.\n",
    "   \n",
    "2. **Model Compilation**:\n",
    "   - The model is compiled using `sparse_categorical_crossentropy` as the loss function, suitable for integer-encoded target labels.\n",
    "   - The Adam optimizer (`'adam'`) is used for gradient descent optimization.\n",
    "   - `'accuracy'` is specified as the metric to monitor during training.\n",
    "\n",
    "3. **Model Summary**:\n",
    "   - The `summary` method is called to display a summary of the model architecture, including the type and shape of each layer, the number of parameters, and the total trainable parameters.\n",
    "\n",
    "4. **Training**:\n",
    "   - The model is trained using the `fit` method, which takes the padded sequences (`padded_sequences`) as input features and the corresponding integer-encoded labels (`training_labels`).\n",
    "   - The number of training epochs (`epochs`) is set to 500.\n",
    "   - The training process updates the model's weights to minimize the specified loss function (sparse categorical cross-entropy) and maximizes the accuracy metric.\n",
    "\n",
    "This architecture is commonly used for text classification tasks, where the model learns to predict the category or label associated with a given input text based on its semantic meaning encoded in the word embeddings.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e45f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 20, 16)            16000     \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 19)                323       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16867 (65.89 KB)\n",
      "Trainable params: 16867 (65.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "3/3 [==============================] - 1s 5ms/step - loss: 2.9447 - accuracy: 0.0462\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9438 - accuracy: 0.0769\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9432 - accuracy: 0.1231\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9429 - accuracy: 0.0923\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9425 - accuracy: 0.0462\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9422 - accuracy: 0.0462\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9416 - accuracy: 0.0462\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9409 - accuracy: 0.0923\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9404 - accuracy: 0.0923\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9396 - accuracy: 0.0462\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9390 - accuracy: 0.0462\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9387 - accuracy: 0.0462\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9381 - accuracy: 0.0615\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9376 - accuracy: 0.0615\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9370 - accuracy: 0.0615\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9364 - accuracy: 0.0769\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9357 - accuracy: 0.0923\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9353 - accuracy: 0.0769\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9348 - accuracy: 0.0615\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9338 - accuracy: 0.0615\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9326 - accuracy: 0.1385\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9319 - accuracy: 0.1538\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9316 - accuracy: 0.1538\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9310 - accuracy: 0.1231\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9302 - accuracy: 0.1077\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9297 - accuracy: 0.1077\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9293 - accuracy: 0.1077\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9286 - accuracy: 0.1077\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9279 - accuracy: 0.0923\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9274 - accuracy: 0.0769\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9269 - accuracy: 0.0615\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9260 - accuracy: 0.0615\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9246 - accuracy: 0.0769\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9229 - accuracy: 0.0769\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9214 - accuracy: 0.1077\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9202 - accuracy: 0.1077\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9191 - accuracy: 0.1538\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9177 - accuracy: 0.1846\n",
      "Epoch 39/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9163 - accuracy: 0.2000\n",
      "Epoch 40/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9147 - accuracy: 0.2154\n",
      "Epoch 41/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9132 - accuracy: 0.2154\n",
      "Epoch 42/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9115 - accuracy: 0.2000\n",
      "Epoch 43/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9099 - accuracy: 0.1846\n",
      "Epoch 44/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.9080 - accuracy: 0.2000\n",
      "Epoch 45/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9062 - accuracy: 0.2154\n",
      "Epoch 46/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9050 - accuracy: 0.1692\n",
      "Epoch 47/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9040 - accuracy: 0.0923\n",
      "Epoch 48/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9022 - accuracy: 0.0769\n",
      "Epoch 49/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.9001 - accuracy: 0.0923\n",
      "Epoch 50/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8984 - accuracy: 0.1231\n",
      "Epoch 51/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 2.8963 - accuracy: 0.1385\n",
      "Epoch 52/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8942 - accuracy: 0.1538\n",
      "Epoch 53/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8913 - accuracy: 0.1538\n",
      "Epoch 54/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8882 - accuracy: 0.1385\n",
      "Epoch 55/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 2.8852 - accuracy: 0.1385\n",
      "Epoch 56/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8816 - accuracy: 0.1385\n",
      "Epoch 57/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8788 - accuracy: 0.1077\n",
      "Epoch 58/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8759 - accuracy: 0.1077\n",
      "Epoch 59/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8731 - accuracy: 0.1077\n",
      "Epoch 60/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8702 - accuracy: 0.1077\n",
      "Epoch 61/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8677 - accuracy: 0.1231\n",
      "Epoch 62/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8648 - accuracy: 0.1385\n",
      "Epoch 63/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8622 - accuracy: 0.1385\n",
      "Epoch 64/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8592 - accuracy: 0.1385\n",
      "Epoch 65/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8561 - accuracy: 0.1385\n",
      "Epoch 66/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8530 - accuracy: 0.1077\n",
      "Epoch 67/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8499 - accuracy: 0.1077\n",
      "Epoch 68/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8469 - accuracy: 0.1077\n",
      "Epoch 69/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8446 - accuracy: 0.1077\n",
      "Epoch 70/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8423 - accuracy: 0.1077\n",
      "Epoch 71/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8387 - accuracy: 0.1077\n",
      "Epoch 72/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8350 - accuracy: 0.1077\n",
      "Epoch 73/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.8320 - accuracy: 0.1077\n",
      "Epoch 74/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8275 - accuracy: 0.1077\n",
      "Epoch 75/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.8222 - accuracy: 0.1077\n",
      "Epoch 76/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8176 - accuracy: 0.1077\n",
      "Epoch 77/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8132 - accuracy: 0.1077\n",
      "Epoch 78/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8083 - accuracy: 0.1077\n",
      "Epoch 79/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.8030 - accuracy: 0.1077\n",
      "Epoch 80/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7980 - accuracy: 0.1077\n",
      "Epoch 81/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7928 - accuracy: 0.1077\n",
      "Epoch 82/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7867 - accuracy: 0.1077\n",
      "Epoch 83/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7811 - accuracy: 0.1077\n",
      "Epoch 84/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.7750 - accuracy: 0.1077\n",
      "Epoch 85/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.7693 - accuracy: 0.1077\n",
      "Epoch 86/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7647 - accuracy: 0.1077\n",
      "Epoch 87/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.7597 - accuracy: 0.1077\n",
      "Epoch 88/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.7534 - accuracy: 0.1077\n",
      "Epoch 89/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.7469 - accuracy: 0.1077\n",
      "Epoch 90/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7404 - accuracy: 0.1385\n",
      "Epoch 91/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.7344 - accuracy: 0.1692\n",
      "Epoch 92/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.7290 - accuracy: 0.1692\n",
      "Epoch 93/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7236 - accuracy: 0.1692\n",
      "Epoch 94/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7187 - accuracy: 0.1692\n",
      "Epoch 95/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7130 - accuracy: 0.1692\n",
      "Epoch 96/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7080 - accuracy: 0.1692\n",
      "Epoch 97/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7026 - accuracy: 0.1692\n",
      "Epoch 98/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.6980 - accuracy: 0.1692\n",
      "Epoch 99/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.6937 - accuracy: 0.1692\n",
      "Epoch 100/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.6904 - accuracy: 0.1692\n",
      "Epoch 101/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.6839 - accuracy: 0.1692\n",
      "Epoch 102/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.6789 - accuracy: 0.1692\n",
      "Epoch 103/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.6715 - accuracy: 0.1692\n",
      "Epoch 104/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.6648 - accuracy: 0.1692\n",
      "Epoch 105/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.6575 - accuracy: 0.1692\n",
      "Epoch 106/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.6508 - accuracy: 0.1692\n",
      "Epoch 107/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.6455 - accuracy: 0.1692\n",
      "Epoch 108/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.6396 - accuracy: 0.1692\n",
      "Epoch 109/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.6321 - accuracy: 0.1692\n",
      "Epoch 110/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.6251 - accuracy: 0.1692\n",
      "Epoch 111/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.6175 - accuracy: 0.1692\n",
      "Epoch 112/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.6098 - accuracy: 0.1692\n",
      "Epoch 113/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.6009 - accuracy: 0.1692\n",
      "Epoch 114/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.5911 - accuracy: 0.1692\n",
      "Epoch 115/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.5830 - accuracy: 0.1692\n",
      "Epoch 116/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.5751 - accuracy: 0.1692\n",
      "Epoch 117/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.5686 - accuracy: 0.1692\n",
      "Epoch 118/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.5614 - accuracy: 0.1692\n",
      "Epoch 119/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.5544 - accuracy: 0.1692\n",
      "Epoch 120/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.5478 - accuracy: 0.1692\n",
      "Epoch 121/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.5408 - accuracy: 0.1692\n",
      "Epoch 122/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.5321 - accuracy: 0.1846\n",
      "Epoch 123/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.5243 - accuracy: 0.2000\n",
      "Epoch 124/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.5165 - accuracy: 0.2154\n",
      "Epoch 125/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.5099 - accuracy: 0.2154\n",
      "Epoch 126/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.5027 - accuracy: 0.2154\n",
      "Epoch 127/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4949 - accuracy: 0.2154\n",
      "Epoch 128/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.4866 - accuracy: 0.2154\n",
      "Epoch 129/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4788 - accuracy: 0.2000\n",
      "Epoch 130/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4712 - accuracy: 0.1846\n",
      "Epoch 131/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4667 - accuracy: 0.1846\n",
      "Epoch 132/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4600 - accuracy: 0.1846\n",
      "Epoch 133/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4524 - accuracy: 0.1846\n",
      "Epoch 134/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.4442 - accuracy: 0.2000\n",
      "Epoch 135/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4363 - accuracy: 0.2154\n",
      "Epoch 136/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4272 - accuracy: 0.2000\n",
      "Epoch 137/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4183 - accuracy: 0.2308\n",
      "Epoch 138/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4133 - accuracy: 0.2462\n",
      "Epoch 139/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4116 - accuracy: 0.2769\n",
      "Epoch 140/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4095 - accuracy: 0.3077\n",
      "Epoch 141/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.4063 - accuracy: 0.3077\n",
      "Epoch 142/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3997 - accuracy: 0.3077\n",
      "Epoch 143/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3882 - accuracy: 0.2923\n",
      "Epoch 144/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3758 - accuracy: 0.2769\n",
      "Epoch 145/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3647 - accuracy: 0.2769\n",
      "Epoch 146/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3541 - accuracy: 0.2769\n",
      "Epoch 147/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3456 - accuracy: 0.2308\n",
      "Epoch 148/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3397 - accuracy: 0.2308\n",
      "Epoch 149/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3322 - accuracy: 0.2308\n",
      "Epoch 150/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3237 - accuracy: 0.2308\n",
      "Epoch 151/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3152 - accuracy: 0.2462\n",
      "Epoch 152/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3072 - accuracy: 0.2462\n",
      "Epoch 153/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3006 - accuracy: 0.2615\n",
      "Epoch 154/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2926 - accuracy: 0.2769\n",
      "Epoch 155/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2836 - accuracy: 0.2769\n",
      "Epoch 156/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2748 - accuracy: 0.2923\n",
      "Epoch 157/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.2650 - accuracy: 0.2923\n",
      "Epoch 158/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2595 - accuracy: 0.2462\n",
      "Epoch 159/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2585 - accuracy: 0.2462\n",
      "Epoch 160/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2479 - accuracy: 0.2462\n",
      "Epoch 161/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2392 - accuracy: 0.2462\n",
      "Epoch 162/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.2327 - accuracy: 0.2462\n",
      "Epoch 163/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2260 - accuracy: 0.2462\n",
      "Epoch 164/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2207 - accuracy: 0.2462\n",
      "Epoch 165/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2152 - accuracy: 0.2462\n",
      "Epoch 166/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2093 - accuracy: 0.2615\n",
      "Epoch 167/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.2029 - accuracy: 0.2769\n",
      "Epoch 168/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1943 - accuracy: 0.2769\n",
      "Epoch 169/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1840 - accuracy: 0.2769\n",
      "Epoch 170/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1727 - accuracy: 0.2769\n",
      "Epoch 171/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1618 - accuracy: 0.2769\n",
      "Epoch 172/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1536 - accuracy: 0.3077\n",
      "Epoch 173/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1449 - accuracy: 0.3077\n",
      "Epoch 174/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.1373 - accuracy: 0.2923\n",
      "Epoch 175/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1311 - accuracy: 0.2923\n",
      "Epoch 176/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1269 - accuracy: 0.2769\n",
      "Epoch 177/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1198 - accuracy: 0.2769\n",
      "Epoch 178/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1114 - accuracy: 0.2769\n",
      "Epoch 179/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.1049 - accuracy: 0.2769\n",
      "Epoch 180/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.0944 - accuracy: 0.2769\n",
      "Epoch 181/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.0830 - accuracy: 0.3077\n",
      "Epoch 182/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.0752 - accuracy: 0.3077\n",
      "Epoch 183/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.0677 - accuracy: 0.3231\n",
      "Epoch 184/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.0590 - accuracy: 0.3231\n",
      "Epoch 185/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.0505 - accuracy: 0.3231\n",
      "Epoch 186/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.0431 - accuracy: 0.3077\n",
      "Epoch 187/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.0361 - accuracy: 0.3077\n",
      "Epoch 188/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.0294 - accuracy: 0.3231\n",
      "Epoch 189/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.0220 - accuracy: 0.3231\n",
      "Epoch 190/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.0169 - accuracy: 0.3385\n",
      "Epoch 191/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 2.0100 - accuracy: 0.3385\n",
      "Epoch 192/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.0031 - accuracy: 0.3231\n",
      "Epoch 193/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.9960 - accuracy: 0.3231\n",
      "Epoch 194/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.9882 - accuracy: 0.3231\n",
      "Epoch 195/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.9811 - accuracy: 0.3385\n",
      "Epoch 196/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.9743 - accuracy: 0.3385\n",
      "Epoch 197/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.9687 - accuracy: 0.3385\n",
      "Epoch 198/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.9616 - accuracy: 0.3538\n",
      "Epoch 199/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.9572 - accuracy: 0.3538\n",
      "Epoch 200/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.9512 - accuracy: 0.3538\n",
      "Epoch 201/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.9452 - accuracy: 0.3538\n",
      "Epoch 202/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.9390 - accuracy: 0.3538\n",
      "Epoch 203/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.9319 - accuracy: 0.3538\n",
      "Epoch 204/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.9252 - accuracy: 0.3538\n",
      "Epoch 205/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.9175 - accuracy: 0.3538\n",
      "Epoch 206/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.9097 - accuracy: 0.3538\n",
      "Epoch 207/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.9014 - accuracy: 0.3538\n",
      "Epoch 208/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8952 - accuracy: 0.3538\n",
      "Epoch 209/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8895 - accuracy: 0.3538\n",
      "Epoch 210/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8829 - accuracy: 0.3385\n",
      "Epoch 211/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.8767 - accuracy: 0.3385\n",
      "Epoch 212/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8703 - accuracy: 0.3385\n",
      "Epoch 213/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8645 - accuracy: 0.3538\n",
      "Epoch 214/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8600 - accuracy: 0.3538\n",
      "Epoch 215/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8539 - accuracy: 0.3538\n",
      "Epoch 216/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8475 - accuracy: 0.3385\n",
      "Epoch 217/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.8440 - accuracy: 0.3385\n",
      "Epoch 218/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8405 - accuracy: 0.3231\n",
      "Epoch 219/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8345 - accuracy: 0.3385\n",
      "Epoch 220/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8242 - accuracy: 0.3385\n",
      "Epoch 221/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8137 - accuracy: 0.3538\n",
      "Epoch 222/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.8078 - accuracy: 0.3538\n",
      "Epoch 223/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.8040 - accuracy: 0.3538\n",
      "Epoch 224/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7976 - accuracy: 0.3538\n",
      "Epoch 225/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7883 - accuracy: 0.3846\n",
      "Epoch 226/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7814 - accuracy: 0.3846\n",
      "Epoch 227/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7751 - accuracy: 0.3846\n",
      "Epoch 228/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7695 - accuracy: 0.3846\n",
      "Epoch 229/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7634 - accuracy: 0.3846\n",
      "Epoch 230/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7570 - accuracy: 0.3846\n",
      "Epoch 231/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7507 - accuracy: 0.3846\n",
      "Epoch 232/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.7458 - accuracy: 0.3846\n",
      "Epoch 233/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7429 - accuracy: 0.3538\n",
      "Epoch 234/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7389 - accuracy: 0.3538\n",
      "Epoch 235/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7327 - accuracy: 0.3538\n",
      "Epoch 236/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7255 - accuracy: 0.3538\n",
      "Epoch 237/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7177 - accuracy: 0.3538\n",
      "Epoch 238/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7106 - accuracy: 0.3538\n",
      "Epoch 239/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.7041 - accuracy: 0.3538\n",
      "Epoch 240/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6987 - accuracy: 0.3538\n",
      "Epoch 241/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6935 - accuracy: 0.3538\n",
      "Epoch 242/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6900 - accuracy: 0.4000\n",
      "Epoch 243/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6862 - accuracy: 0.4000\n",
      "Epoch 244/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6790 - accuracy: 0.4000\n",
      "Epoch 245/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6717 - accuracy: 0.4000\n",
      "Epoch 246/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6659 - accuracy: 0.4000\n",
      "Epoch 247/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6598 - accuracy: 0.4000\n",
      "Epoch 248/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6544 - accuracy: 0.3846\n",
      "Epoch 249/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6494 - accuracy: 0.4000\n",
      "Epoch 250/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6430 - accuracy: 0.3846\n",
      "Epoch 251/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6393 - accuracy: 0.3846\n",
      "Epoch 252/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6336 - accuracy: 0.4000\n",
      "Epoch 253/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6285 - accuracy: 0.4000\n",
      "Epoch 254/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6244 - accuracy: 0.4154\n",
      "Epoch 255/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6219 - accuracy: 0.4000\n",
      "Epoch 256/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6181 - accuracy: 0.3846\n",
      "Epoch 257/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6158 - accuracy: 0.4000\n",
      "Epoch 258/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6139 - accuracy: 0.3846\n",
      "Epoch 259/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.6071 - accuracy: 0.4154\n",
      "Epoch 260/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5988 - accuracy: 0.4462\n",
      "Epoch 261/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5892 - accuracy: 0.4462\n",
      "Epoch 262/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5826 - accuracy: 0.4615\n",
      "Epoch 263/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5782 - accuracy: 0.4769\n",
      "Epoch 264/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5739 - accuracy: 0.4769\n",
      "Epoch 265/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5710 - accuracy: 0.4308\n",
      "Epoch 266/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5665 - accuracy: 0.4462\n",
      "Epoch 267/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5617 - accuracy: 0.4615\n",
      "Epoch 268/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5583 - accuracy: 0.4615\n",
      "Epoch 269/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5559 - accuracy: 0.4615\n",
      "Epoch 270/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5521 - accuracy: 0.4615\n",
      "Epoch 271/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5454 - accuracy: 0.4615\n",
      "Epoch 272/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5387 - accuracy: 0.5077\n",
      "Epoch 273/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5313 - accuracy: 0.5231\n",
      "Epoch 274/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5235 - accuracy: 0.5385\n",
      "Epoch 275/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5156 - accuracy: 0.5385\n",
      "Epoch 276/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.5058 - accuracy: 0.5077\n",
      "Epoch 277/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4990 - accuracy: 0.4923\n",
      "Epoch 278/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4945 - accuracy: 0.5077\n",
      "Epoch 279/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4900 - accuracy: 0.5231\n",
      "Epoch 280/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4833 - accuracy: 0.5385\n",
      "Epoch 281/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4751 - accuracy: 0.5692\n",
      "Epoch 282/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4678 - accuracy: 0.5538\n",
      "Epoch 283/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4614 - accuracy: 0.5538\n",
      "Epoch 284/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4568 - accuracy: 0.5231\n",
      "Epoch 285/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4518 - accuracy: 0.4923\n",
      "Epoch 286/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4456 - accuracy: 0.5077\n",
      "Epoch 287/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4399 - accuracy: 0.5231\n",
      "Epoch 288/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4334 - accuracy: 0.5231\n",
      "Epoch 289/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4275 - accuracy: 0.5538\n",
      "Epoch 290/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4218 - accuracy: 0.5538\n",
      "Epoch 291/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4167 - accuracy: 0.5538\n",
      "Epoch 292/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4115 - accuracy: 0.5538\n",
      "Epoch 293/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4066 - accuracy: 0.5538\n",
      "Epoch 294/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.4014 - accuracy: 0.5538\n",
      "Epoch 295/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3964 - accuracy: 0.5385\n",
      "Epoch 296/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3917 - accuracy: 0.5538\n",
      "Epoch 297/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3856 - accuracy: 0.5538\n",
      "Epoch 298/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3796 - accuracy: 0.5538\n",
      "Epoch 299/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3746 - accuracy: 0.5538\n",
      "Epoch 300/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.3694 - accuracy: 0.5538\n",
      "Epoch 301/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3655 - accuracy: 0.5538\n",
      "Epoch 302/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3613 - accuracy: 0.5692\n",
      "Epoch 303/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3561 - accuracy: 0.5692\n",
      "Epoch 304/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3499 - accuracy: 0.5692\n",
      "Epoch 305/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3455 - accuracy: 0.5692\n",
      "Epoch 306/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3397 - accuracy: 0.5846\n",
      "Epoch 307/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3342 - accuracy: 0.5846\n",
      "Epoch 308/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3298 - accuracy: 0.6000\n",
      "Epoch 309/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3242 - accuracy: 0.6000\n",
      "Epoch 310/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3203 - accuracy: 0.6154\n",
      "Epoch 311/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3181 - accuracy: 0.6308\n",
      "Epoch 312/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.3122 - accuracy: 0.6462\n",
      "Epoch 313/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.3050 - accuracy: 0.6462\n",
      "Epoch 314/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2978 - accuracy: 0.6462\n",
      "Epoch 315/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2929 - accuracy: 0.6615\n",
      "Epoch 316/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2889 - accuracy: 0.6769\n",
      "Epoch 317/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2856 - accuracy: 0.6769\n",
      "Epoch 318/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2801 - accuracy: 0.6615\n",
      "Epoch 319/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2768 - accuracy: 0.6615\n",
      "Epoch 320/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2709 - accuracy: 0.6615\n",
      "Epoch 321/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2628 - accuracy: 0.6615\n",
      "Epoch 322/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2573 - accuracy: 0.6615\n",
      "Epoch 323/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2532 - accuracy: 0.6769\n",
      "Epoch 324/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2489 - accuracy: 0.6462\n",
      "Epoch 325/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2437 - accuracy: 0.6462\n",
      "Epoch 326/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2392 - accuracy: 0.6462\n",
      "Epoch 327/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2324 - accuracy: 0.6462\n",
      "Epoch 328/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2296 - accuracy: 0.6615\n",
      "Epoch 329/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2256 - accuracy: 0.6462\n",
      "Epoch 330/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2212 - accuracy: 0.6615\n",
      "Epoch 331/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2178 - accuracy: 0.6769\n",
      "Epoch 332/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.2109 - accuracy: 0.6615\n",
      "Epoch 333/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.2007 - accuracy: 0.6615\n",
      "Epoch 334/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1927 - accuracy: 0.6769\n",
      "Epoch 335/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1889 - accuracy: 0.6923\n",
      "Epoch 336/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1856 - accuracy: 0.6923\n",
      "Epoch 337/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1784 - accuracy: 0.6923\n",
      "Epoch 338/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1726 - accuracy: 0.6923\n",
      "Epoch 339/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1669 - accuracy: 0.6923\n",
      "Epoch 340/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1620 - accuracy: 0.6923\n",
      "Epoch 341/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1576 - accuracy: 0.6923\n",
      "Epoch 342/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1533 - accuracy: 0.6923\n",
      "Epoch 343/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1500 - accuracy: 0.6923\n",
      "Epoch 344/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1479 - accuracy: 0.6923\n",
      "Epoch 345/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1457 - accuracy: 0.6923\n",
      "Epoch 346/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.1417 - accuracy: 0.6923\n",
      "Epoch 347/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1351 - accuracy: 0.6923\n",
      "Epoch 348/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1307 - accuracy: 0.6923\n",
      "Epoch 349/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.1287 - accuracy: 0.6769\n",
      "Epoch 350/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1242 - accuracy: 0.6923\n",
      "Epoch 351/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.1172 - accuracy: 0.7231\n",
      "Epoch 352/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 1.1111 - accuracy: 0.7077\n",
      "Epoch 353/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1053 - accuracy: 0.7077\n",
      "Epoch 354/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.1031 - accuracy: 0.7077\n",
      "Epoch 355/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0979 - accuracy: 0.7077\n",
      "Epoch 356/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0930 - accuracy: 0.7077\n",
      "Epoch 357/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0882 - accuracy: 0.7077\n",
      "Epoch 358/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0827 - accuracy: 0.7077\n",
      "Epoch 359/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0769 - accuracy: 0.7077\n",
      "Epoch 360/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0731 - accuracy: 0.7077\n",
      "Epoch 361/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0699 - accuracy: 0.7077\n",
      "Epoch 362/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0676 - accuracy: 0.7077\n",
      "Epoch 363/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.0608 - accuracy: 0.7385\n",
      "Epoch 364/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0568 - accuracy: 0.7385\n",
      "Epoch 365/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0549 - accuracy: 0.7538\n",
      "Epoch 366/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0535 - accuracy: 0.7538\n",
      "Epoch 367/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0518 - accuracy: 0.7692\n",
      "Epoch 368/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0481 - accuracy: 0.7846\n",
      "Epoch 369/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0426 - accuracy: 0.7846\n",
      "Epoch 370/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.0379 - accuracy: 0.8000\n",
      "Epoch 371/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0366 - accuracy: 0.8308\n",
      "Epoch 372/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0334 - accuracy: 0.8308\n",
      "Epoch 373/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0282 - accuracy: 0.8308\n",
      "Epoch 374/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0180 - accuracy: 0.8308\n",
      "Epoch 375/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0212 - accuracy: 0.8308\n",
      "Epoch 376/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0232 - accuracy: 0.8154\n",
      "Epoch 377/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0150 - accuracy: 0.8154\n",
      "Epoch 378/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0058 - accuracy: 0.8000\n",
      "Epoch 379/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9990 - accuracy: 0.7538\n",
      "Epoch 380/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9979 - accuracy: 0.7692\n",
      "Epoch 381/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9983 - accuracy: 0.7538\n",
      "Epoch 382/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9994 - accuracy: 0.7385\n",
      "Epoch 383/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.0004 - accuracy: 0.7692\n",
      "Epoch 384/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9963 - accuracy: 0.7538\n",
      "Epoch 385/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9877 - accuracy: 0.7538\n",
      "Epoch 386/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9796 - accuracy: 0.7846\n",
      "Epoch 387/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9768 - accuracy: 0.7692\n",
      "Epoch 388/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9730 - accuracy: 0.7692\n",
      "Epoch 389/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9729 - accuracy: 0.8000\n",
      "Epoch 390/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9749 - accuracy: 0.8154\n",
      "Epoch 391/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9690 - accuracy: 0.8154\n",
      "Epoch 392/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9601 - accuracy: 0.8000\n",
      "Epoch 393/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9537 - accuracy: 0.8000\n",
      "Epoch 394/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9489 - accuracy: 0.8000\n",
      "Epoch 395/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9435 - accuracy: 0.8000\n",
      "Epoch 396/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9365 - accuracy: 0.8308\n",
      "Epoch 397/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9331 - accuracy: 0.8154\n",
      "Epoch 398/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9304 - accuracy: 0.8154\n",
      "Epoch 399/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9277 - accuracy: 0.8308\n",
      "Epoch 400/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9252 - accuracy: 0.8308\n",
      "Epoch 401/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9216 - accuracy: 0.8000\n",
      "Epoch 402/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.9197 - accuracy: 0.8000\n",
      "Epoch 403/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.9161 - accuracy: 0.7846\n",
      "Epoch 404/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9128 - accuracy: 0.7846\n",
      "Epoch 405/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9101 - accuracy: 0.7846\n",
      "Epoch 406/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9077 - accuracy: 0.7846\n",
      "Epoch 407/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9024 - accuracy: 0.7846\n",
      "Epoch 408/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8955 - accuracy: 0.8308\n",
      "Epoch 409/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8915 - accuracy: 0.8308\n",
      "Epoch 410/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8886 - accuracy: 0.8308\n",
      "Epoch 411/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8850 - accuracy: 0.8308\n",
      "Epoch 412/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8855 - accuracy: 0.8308\n",
      "Epoch 413/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8842 - accuracy: 0.8154\n",
      "Epoch 414/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8814 - accuracy: 0.8154\n",
      "Epoch 415/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8774 - accuracy: 0.8462\n",
      "Epoch 416/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8751 - accuracy: 0.8154\n",
      "Epoch 417/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8751 - accuracy: 0.8154\n",
      "Epoch 418/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8738 - accuracy: 0.8154\n",
      "Epoch 419/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8719 - accuracy: 0.8000\n",
      "Epoch 420/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8678 - accuracy: 0.8000\n",
      "Epoch 421/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8626 - accuracy: 0.8154\n",
      "Epoch 422/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8558 - accuracy: 0.8615\n",
      "Epoch 423/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8498 - accuracy: 0.8769\n",
      "Epoch 424/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8458 - accuracy: 0.8769\n",
      "Epoch 425/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8426 - accuracy: 0.8769\n",
      "Epoch 426/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8386 - accuracy: 0.8769\n",
      "Epoch 427/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8331 - accuracy: 0.8769\n",
      "Epoch 428/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8284 - accuracy: 0.8769\n",
      "Epoch 429/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8247 - accuracy: 0.8615\n",
      "Epoch 430/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8262 - accuracy: 0.8154\n",
      "Epoch 431/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8236 - accuracy: 0.8154\n",
      "Epoch 432/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8208 - accuracy: 0.8308\n",
      "Epoch 433/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8146 - accuracy: 0.8308\n",
      "Epoch 434/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8063 - accuracy: 0.8308\n",
      "Epoch 435/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8014 - accuracy: 0.8462\n",
      "Epoch 436/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7968 - accuracy: 0.8615\n",
      "Epoch 437/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7937 - accuracy: 0.8462\n",
      "Epoch 438/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7900 - accuracy: 0.8462\n",
      "Epoch 439/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7878 - accuracy: 0.8615\n",
      "Epoch 440/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7855 - accuracy: 0.8615\n",
      "Epoch 441/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7835 - accuracy: 0.8462\n",
      "Epoch 442/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7821 - accuracy: 0.8308\n",
      "Epoch 443/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7788 - accuracy: 0.8308\n",
      "Epoch 444/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7768 - accuracy: 0.8308\n",
      "Epoch 445/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7748 - accuracy: 0.8308\n",
      "Epoch 446/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7724 - accuracy: 0.8615\n",
      "Epoch 447/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7709 - accuracy: 0.8769\n",
      "Epoch 448/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7679 - accuracy: 0.8769\n",
      "Epoch 449/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7652 - accuracy: 0.8615\n",
      "Epoch 450/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7622 - accuracy: 0.8615\n",
      "Epoch 451/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7594 - accuracy: 0.8769\n",
      "Epoch 452/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7611 - accuracy: 0.8615\n",
      "Epoch 453/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7658 - accuracy: 0.8615\n",
      "Epoch 454/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7680 - accuracy: 0.8462\n",
      "Epoch 455/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7628 - accuracy: 0.8462\n",
      "Epoch 456/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7518 - accuracy: 0.8462\n",
      "Epoch 457/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7511 - accuracy: 0.8462\n",
      "Epoch 458/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7513 - accuracy: 0.8154\n",
      "Epoch 459/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7581 - accuracy: 0.8154\n",
      "Epoch 460/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7571 - accuracy: 0.8000\n",
      "Epoch 461/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7549 - accuracy: 0.7692\n",
      "Epoch 462/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7479 - accuracy: 0.7846\n",
      "Epoch 463/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7378 - accuracy: 0.8154\n",
      "Epoch 464/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7293 - accuracy: 0.8154\n",
      "Epoch 465/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7222 - accuracy: 0.8462\n",
      "Epoch 466/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7214 - accuracy: 0.8462\n",
      "Epoch 467/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7196 - accuracy: 0.8615\n",
      "Epoch 468/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7193 - accuracy: 0.8462\n",
      "Epoch 469/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7207 - accuracy: 0.8615\n",
      "Epoch 470/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7226 - accuracy: 0.8462\n",
      "Epoch 471/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7252 - accuracy: 0.8308\n",
      "Epoch 472/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7273 - accuracy: 0.7846\n",
      "Epoch 473/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7321 - accuracy: 0.7692\n",
      "Epoch 474/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7328 - accuracy: 0.7846\n",
      "Epoch 475/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7270 - accuracy: 0.7846\n",
      "Epoch 476/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7193 - accuracy: 0.8615\n",
      "Epoch 477/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7122 - accuracy: 0.9077\n",
      "Epoch 478/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7062 - accuracy: 0.9077\n",
      "Epoch 479/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7014 - accuracy: 0.9077\n",
      "Epoch 480/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6942 - accuracy: 0.9385\n",
      "Epoch 481/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6893 - accuracy: 0.9538\n",
      "Epoch 482/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6848 - accuracy: 0.9385\n",
      "Epoch 483/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.9077\n",
      "Epoch 484/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6822 - accuracy: 0.8769\n",
      "Epoch 485/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6801 - accuracy: 0.8923\n",
      "Epoch 486/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6756 - accuracy: 0.8923\n",
      "Epoch 487/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6678 - accuracy: 0.9231\n",
      "Epoch 488/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6631 - accuracy: 0.9231\n",
      "Epoch 489/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6573 - accuracy: 0.9231\n",
      "Epoch 490/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6545 - accuracy: 0.9385\n",
      "Epoch 491/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6534 - accuracy: 0.9385\n",
      "Epoch 492/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6507 - accuracy: 0.9385\n",
      "Epoch 493/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6482 - accuracy: 0.9385\n",
      "Epoch 494/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.9385\n",
      "Epoch 495/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6413 - accuracy: 0.9538\n",
      "Epoch 496/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6394 - accuracy: 0.9538\n",
      "Epoch 497/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6391 - accuracy: 0.9538\n",
      "Epoch 498/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6425 - accuracy: 0.9385\n",
      "Epoch 499/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.9385\n",
      "Epoch 500/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6342 - accuracy: 0.9385\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length = max_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "             optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "epochs = 500\n",
    "history = model.fit(padded_sequences, np.array(training_labels), epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b4fcd",
   "metadata": {},
   "source": [
    "The provided code snippet saves the trained model to a file named \"chatbot1_model\". This action is crucial for preserving the trained model's architecture, weights, and configuration so that it can be reused later for inference or further training without needing to retrain the model from scratch.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c456b01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chatbot1_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chatbot1_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"chatbot1_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e373deb3",
   "metadata": {},
   "source": [
    "The code snippet saves the Tokenizer object used for tokenizing text data to a file named \"tokenizer.pickle\". This object contains the vocabulary built during training, as well as the mapping between words and their corresponding indices. Saving the tokenizer allows for consistency in text preprocessing during inference or when using the model for prediction on new data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a4044d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b7257",
   "metadata": {},
   "source": [
    "The `colorama` library is imported and initialized to enable colored output in the terminal. This allows for visually distinguishing different types of messages or highlighting specific elements during chatbot interactions. The `Fore`, `Style`, and `Back` modules from `colorama` provide support for foreground, style, and background colors, respectively. Additionally, the `random` module is imported to facilitate randomization, which can be useful for generating varied responses or selecting elements randomly during chat interactions.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "184985ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorama\n",
    "colorama.init\n",
    "from colorama import Fore, Style, Back\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4fb54c",
   "metadata": {},
   "source": [
    "The code snippet reads data from a JSON file named `chatbot_data.json` using a context manager to ensure proper file handling. The JSON data is then loaded into a Python dictionary named `data`, which can be accessed and manipulated within the program. This JSON data likely contains information such as training sentences, labels, and responses for a chatbot.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17d6d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chatbot_data.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b213",
   "metadata": {},
   "source": [
    "The provided code imports necessary libraries and modules for building a chatbot and creating a graphical user interface using Tkinter. Here's a breakdown of the imports:\n",
    "\n",
    "- `json`: Used for working with JSON data.\n",
    "- `tkinter`: A standard GUI (Graphical User Interface) toolkit for Python.\n",
    "- `spacy`: An open-source natural language processing library used for various NLP tasks.\n",
    "- `asyncio`: Provides tools for building asynchronous programs in Python.\n",
    "- `string`: Provides a collection of string constants and helper functions.\n",
    "- `requests`: Allows making HTTP requests.\n",
    "- `textwrap`: Provides functions for formatting text paragraphs.\n",
    "- `pycountry`: Provides country-related functionalities, such as lookup by country name or code.\n",
    "- `wikipediaapi`: A Python wrapper for the Wikipedia API.\n",
    "- `re`: Provides support for regular expressions.\n",
    "- `tensorflow`: An open-source machine learning framework for building and training models.\n",
    "- `numpy`: A library for numerical computations in Python.\n",
    "- `pickle`: Used for serializing and deserializing Python objects.\n",
    "- `ttk`: A module in `tkinter` providing themed widget classes.\n",
    "- `bs4` (Beautiful Soup): A library for pulling data out of HTML and XML files.\n",
    "\n",
    "Additionally, specific functions and classes are imported from some of these libraries, such as `Translator` from `googletrans`, `datetime` from `datetime`, `detect` from `langdetect`, `partial` from `functools`, and `BeautifulSoup` from `bs4`. The code also loads the English language model (`en_core_web_sm`) from SpaCy.\n",
    "\n",
    "Overall, these imports provide the necessary tools and functionalities to implement various features of the chatbot and its user interface.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6634670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tkinter as tk\n",
    "import spacy\n",
    "import asyncio\n",
    "import string\n",
    "import requests\n",
    "import textwrap\n",
    "import pycountry\n",
    "import wikipediaapi\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tkinter import ttk, END\n",
    "from tensorflow import keras\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "from datetime import datetime, date, timedelta\n",
    "from functools import partial\n",
    "from tkinter import ttk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "translator = Translator()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d389f",
   "metadata": {},
   "source": [
    "This section of code defines various functions that handle different aspects of the chat window functionality. Let's break down what each function does:\n",
    "\n",
    "1. `get_location_data(ip_address)`: Retrieves location data based on the user's IP address.\n",
    "2. `get_entity(user_input)`: Uses spaCy to extract named entities from user input.\n",
    "3. `check_status(text)`: Checks for dates in the provided text and returns them.\n",
    "4. `get_news_from_klix()`, `get_news_from_cnn()`, `get_news_from_nyt()`: Retrieve news headlines from different sources.\n",
    "5. `get_news_output(array)`: Processes and displays news headlines.\n",
    "6. `get_random_joke(text)`: Retrieves a random joke based on specified keywords.\n",
    "7. `chat_position_refresh()`: Refreshes the chat position and sets focus on the user input.\n",
    "8. `split_sentence(sentence)`: Splits a sentence and extracts country names using spaCy.\n",
    "9. `find_country_by_substring(substring)`: Searches for a country name containing the specified substring.\n",
    "10. `get_wiki_info(names, user_input)`: Retrieves Wikipedia information based on detected named entities.\n",
    "11. `get_chat_output(text)`: Displays chat output messages with timestamps.\n",
    "12. `get_current_weather(api_key, found_city, trazeni_datum)`: Retrieves current weather data for a given city and date.\n",
    "13. `check_weather(api_key, found_city, date, text)`: Checks weather conditions based on specified criteria.\n",
    "14. `get_weather_info(api_key, found_city, date)`: Retrieves weather forecast data for a given city and date.\n",
    "15. `get_wiki_answer(text)`: Retrieves Wikipedia information for a given query.\n",
    "16. `send_message1(event=None)`: Processes user input, performs various actions based on keywords, and generates appropriate responses.\n",
    "\n",
    "17. `analyze_user_input(user_input, string)`: Analyzes the user's input to check if a specific string is present in it. It uses spaCy's natural language processing capabilities to tokenize the input text and compares each token with the provided string, ignoring case sensitivity. If the string is found, it returns True; otherwise, it returns False.\n",
    "\n",
    "18. `check_if_subject(user_input)`: Checks if the user input contains a subject. It identifies the subject by searching for tokens with the dependency label \"nsubj\" (subject) and specific parts of speech (proper noun, noun, adjective, or pronoun). If a subject is found, it returns the text of the subject token; otherwise, it returns None.\n",
    "\n",
    "19. `get_names(user_input)`: Extracts named entities (specifically, persons) from the user's input. It utilizes spaCy's named entity recognition (NER) capabilities to identify entities labeled as \"PERSON\" in the input text. It then cleans each detected name using the clean_name function and returns a list of cleaned names.\n",
    "\n",
    "20. `clean_name(text)`: Cleans a name by removing any tokens categorized as verbs. It tokenizes the input text using spaCy and retains only those tokens that are not verbs, joining them back into a cleaned string.\n",
    "\n",
    "21. `on_mouse_wheel(event)`: Handles the mouse wheel event, specifically scrolling the canvas of the chat window in response to the mouse wheel movement.\n",
    "\n",
    "22. `remove_placeholder(event)`: Removes the placeholder text (\"Write your message here...\") from the user input field when the field gains focus. It ensures that the placeholder text is cleared to allow the user to input their message without manual deletion.\n",
    "\n",
    "23. `on_enter(event)`: Triggers the send_message1() function when the Enter key is pressed while focusing on the user input field. It allows the user to send their message by pressing Enter instead of clicking a send button.\n",
    "\n",
    "24. `ignore_enter(event)`: Prevents the default behavior of the Enter key when pressed, effectively ignoring the Enter key press event. It ensures that pressing Enter does not trigger any unintended actions.\n",
    "\n",
    "25. `update_scrollregion(event=None)`: Updates the scroll region of the canvas in response to changes in the chat window's size. It ensures that the scrolling functionality adapts dynamically to the chat window's dimensions.\n",
    "\n",
    "\n",
    "\n",
    "These functions collectively manage the chat window's behavior, handling tasks such as fetching news, weather information, jokes, named entity recognition, and Wikipedia queries. They contribute to providing a dynamic and engaging conversational experience for users.\n",
    "\n",
    "**Warning:**\n",
    "Before proceeding, ensure you have set up an API key for weather data access. Failure to do so may result in errors or restricted functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e5f61ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Width for wrapping: ['Hello']\n",
      "Hello\n",
      "User's current location: Mostar\n",
      "User_input Hello\n",
      "2024-05-26\n",
      "0 0\n",
      "what's the weather today?\n",
      "Width for wrapping: [\"what's the weather today?\"]\n",
      "what's the weather today?\n",
      "User's current location: Mostar\n",
      "User_input what's the weather today?\n",
      "Subject-.. weather\n",
      "2024-05-26\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "def open_chat_window(selected_language):\n",
    "    selected_language=selected_language\n",
    "    \n",
    "    async def translate_text(text, src_lang, dest_lang):\n",
    "        translation = await translator.translate(text, src=src_lang, dest=dest_lang)\n",
    "        return translation.text\n",
    "    \n",
    "    def translate_to_analysis(translated_input):\n",
    "        translated_input_en =  translator.translate(translated_input, src=selected_language, dest='en').text\n",
    "        return translated_input_en\n",
    "    \n",
    "    def get_city(user_input):\n",
    "        json_file_path = \"all_cities.json\"\n",
    "        with open(json_file_path, \"r\") as json_file:\n",
    "            city_data = json.load(json_file)\n",
    "        found_city = \"\"\n",
    "        user_words = [word.strip(string.punctuation) for word in user_input.lower().split()]    \n",
    "        for city in city_data:\n",
    "            for word in user_words:\n",
    "                if city['name'].lower() == word:\n",
    "                    found_city = city['name']\n",
    "                    return found_city\n",
    "        return None\n",
    "    \n",
    "    def get_language_code(language_name):\n",
    "        file_path='languages.json'\n",
    "        with open(file_path, 'r') as lang_file:\n",
    "            language_data = json.load(lang_file)\n",
    "\n",
    "        for language in language_data[\"languages\"]:\n",
    "            if language[\"name\"].lower() == language_name.lower():\n",
    "                return language[\"code\"]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def get_country_code(country_name):\n",
    "        try:\n",
    "            country = pycountry.countries.search_fuzzy(country_name)\n",
    "            return country[0].alpha_2\n",
    "        except LookupError:\n",
    "            return None\n",
    "        \n",
    "    def translate_input(user_input,detected_language):\n",
    "        translated=  translator.translate(user_input, src=detected_language, dest=selected_language).text\n",
    "        return translate_to_analysis(translated)\n",
    "    \n",
    "    def get_country_from_ip(ip_address):\n",
    "        url = f\"http://ip-api.com/json/{ip_address}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        if data[\"status\"] == \"success\":\n",
    "            return data[\"country\"]\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "    def get_ip_address():\n",
    "        response = requests.get('https://api.ipify.org')\n",
    "        return response.text\n",
    "    \n",
    "    def get_location_data(ip_address):\n",
    "        url = f\"http://ip-api.com/json/{ip_address}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        return data\n",
    "    \n",
    "    def get_entity(user_input):\n",
    "        names=[]\n",
    "        doc = nlp(user_input)\n",
    "        for ent in doc.ents:\n",
    "                names.append(ent.text)\n",
    "        return names\n",
    "\n",
    "    def check_status(text):\n",
    "        #date format for dead people is ( dd/mm/year) based on wikipedia results\n",
    "        date_pattern1 = r'\\b\\d{1,2}\\s(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}\\b'\n",
    "        #date format for living people is (mm/dd/year) based on wikipedia results\n",
    "        date_pattern2 = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}'\n",
    "    \n",
    "        found_dates=[]\n",
    "        found_dates=re.findall(date_pattern1,text)\n",
    "        if len(found_dates)==0:\n",
    "            found_dates=re.findall(date_pattern2,text)\n",
    "\n",
    "        return found_dates;\n",
    "    \n",
    "    def get_news_from_klix():\n",
    "        \"\"\"\n",
    "        Retrieves local news due to slow or unreliable performance of external APIs, \n",
    "        or their inability to fetch news from local sources.\n",
    "        \"\"\"\n",
    "        url = 'https://www.klix.ba/'\n",
    "        response = requests.get(url)\n",
    "        content = response.content.decode('utf-8') \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        titles = soup.find_all('img', alt=True)\n",
    "        alt_texts = []\n",
    "        for title in titles:\n",
    "            alt_texts.append(title['alt'])\n",
    "        return alt_texts\n",
    "    \n",
    "    def get_news_output(array):\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\") \n",
    "        titles=array[:10]\n",
    "        message = \"\\n\".join(t for t in titles)\n",
    "       # get_chat_output(message)\n",
    "        max_width = 20 \n",
    "        wrapped_message = textwrap.fill(message, width=max_width, break_long_words=False)\n",
    "        time_label = tk.Label(chat_inner_frame, text=current_time, font=(\"Arial\", 8), fg=\"black\")\n",
    "        time_label.pack(anchor=\"n\", padx=5, pady=(10, 0))\n",
    "        button_bg = \"#00008B\"  \n",
    "        foreground = \"white\"\n",
    "        lines = textwrap.wrap(wrapped_message, width=max_width)\n",
    "        label = tk.Label(chat_inner_frame, text=message, font=(\"Arial\", 10), bg=button_bg, fg=foreground, padx=5, pady=5, wraplength=200, justify=tk.LEFT, anchor='w')\n",
    "        label.pack(anchor=\"e\", padx=5, pady=5)\n",
    "        chat_position_refresh()\n",
    " \n",
    "            \n",
    "            \n",
    "    #cnn news            \n",
    "    \n",
    "    def get_news_from_cnn():\n",
    "        url = \"https://edition.cnn.com/\"\n",
    "        clean_headlines=[]\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            headlines = soup.find_all(class_='container__headline')\n",
    "            for headline in headlines:\n",
    "                headline_text = headline.text.strip()\n",
    "                clean_headlines.append(headline_text)\n",
    "            return clean_headlines\n",
    "        else:\n",
    "            print(\"Failed to retrieve news from CNN.\")\n",
    "        \n",
    "    \n",
    "    #new york times headlines\n",
    "    def get_news_from_nyt():\n",
    "        url = 'https://www.nytimes.com/'\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        headlines = soup.find_all('p', class_='indicate-hover')\n",
    "        clean_headlines=[]\n",
    "        for headline in headlines:\n",
    "            clean_headlines.append(headline.text.strip())\n",
    "        return clean_headlines\n",
    "    \n",
    "    def get_random_joke(text):\n",
    "        keywords = [\"Programming\", \"programmers\", \"code\",\"coding\",\"developer\"]\n",
    "        code = 1 if any(analyze_user_input(text, keyword) for keyword in keywords) else 0\n",
    "        if code:\n",
    "            url = \"https://v2.jokeapi.dev/joke/Programming?blacklistFlags=nsfw,religious,political,racist,sexist,explicit\"\n",
    "        else:\n",
    "            # Take all jokes except ones blacklisted ones ( religious, political)\n",
    "            url = \"https://v2.jokeapi.dev/joke/Any?blacklistFlags=nsfw,religious,political,racist,sexist,explicit\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            joke_data = response.json()\n",
    "            if 'joke' in joke_data:\n",
    "                return joke_data['joke']\n",
    "            elif 'setup' in joke_data and 'delivery' in joke_data:\n",
    "                return f\"{joke_data['setup']} {joke_data['delivery']}\"\n",
    "        else:\n",
    "            return \"It is not possible to fetch the joke.\"\n",
    "\n",
    "    def chat_position_refresh():\n",
    "        \"\"\"\n",
    "        Refreshes the chat position.\n",
    "        By moving the canvas downwards, ensures that new messages are visible.\n",
    "        Sets focus on the user input and places the cursor at the beginning of the text.\n",
    "        \"\"\"\n",
    "        update_scrollregion() \n",
    "        canvas.yview_moveto(1.0)  \n",
    "        canvas.yview_moveto(1.0)  \n",
    "        user_input.after(10, lambda: user_input.mark_set(tk.INSERT, \"1.0\"))\n",
    "        canvas.update_idletasks()  \n",
    "        user_input.focus_set()\n",
    "        user_input.mark_set(tk.INSERT, \"1.0\")  \n",
    "        user_input.after(10, lambda: user_input.mark_set(tk.INSERT, \"1.0\"))\n",
    "        canvas.update_idletasks()  \n",
    "        \n",
    "    def split_sentence(sentence):\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'ADJ':\n",
    "                country_name = find_country_by_substring(token.text)\n",
    "                if country_name:\n",
    "                    return token\n",
    "        return None\n",
    "    \n",
    "    def find_country_by_substring(substring):\n",
    "        countries=pycountry.countries\n",
    "        for country in countries:\n",
    "            if substring.lower() in country.name.lower():\n",
    "                return country.name\n",
    "        return None;\n",
    "    \n",
    "    def get_wiki_info(names, user_input):        \n",
    "        wiki_wiki = wikipediaapi.Wikipedia(\n",
    "                language=\"en\",\n",
    "                extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "                user_agent='YourAppName/1.0 (YourContactInfo)'\n",
    "            )\n",
    "\n",
    "        alive= 1 if analyze_user_input(user_input,\"alive\") else 0\n",
    "        nationality=1 if analyze_user_input(user_input,\"nationality\") or analyze_user_input(user_input,\"origin\")  else 0\n",
    "        user_info=1 if analyze_user_input(user_input,\"is\") or analyze_user_input(user_input,\"was\") else 0\n",
    "        for name in names:\n",
    "            person_name = name\n",
    "            page = wiki_wiki.page(person_name)\n",
    "            dates=[]\n",
    "            person_name_upper = person_name.title()\n",
    "\n",
    "            if page.exists():\n",
    "                print(\"Full Name: \" + page.title)\n",
    "                print(\"Summary:\")\n",
    "                sentences = re.split(r'(?<=[.!?])\\s+', page.text)\n",
    "               \n",
    "                short_summary = '.'.join(sentences[:5])\n",
    "                \n",
    "                #for dates (born-death)\n",
    "                if (selected_language=='en'):\n",
    "                    first_sentence = short_summary.split('.')[0]\n",
    "                else:\n",
    "                    first_sentence = re.split(r'(\\d{1,2}\\.\\d{1,2}\\.\\d{4})', short_summary)[0]\n",
    "                translated_text = translator.translate(short_summary, src=selected_language, dest='en').text\n",
    "                first_sentence = re.split(r'(\\d{1,2}\\.\\d{1,2}\\.\\d{4})', translated_text)[0]\n",
    "               \n",
    "                \n",
    "                if user_info==1 and alive==0 and nationality==0:\n",
    "                    if selected_language != 'english':\n",
    "                        short_summary=translated_text = translator.translate(short_summary, src='en', dest=selected_language).text\n",
    "                    get_chat_output(short_summary)\n",
    "                if(alive==1):\n",
    "                    dates=check_status(first_sentence)\n",
    "        \n",
    "                    if dates:\n",
    "                        if len(dates)>1:\n",
    "                            translated_text = translator.translate(f\"No, {person_name_upper} is not alive.\\n {person_name_upper} died on {dates[1]}\", src='en', dest=selected_language).text\n",
    "                        else:\n",
    "                            translated_text = translator.translate(f\"Yes, {person_name_upper} is alive.\\n {person_name_upper} is born on {dates[0]}\", src='en', dest=selected_language).text\n",
    "                        if translated_text:\n",
    "                            get_chat_output(translated_text)\n",
    "                if(nationality==1):\n",
    "                    country_name=split_sentence(first_sentence)\n",
    "                    if country_name:\n",
    "                        translated_text = translator.translate(f\"{person_name_upper} nationality is {country_name}.\\n\", src='en', dest=selected_language).text\n",
    "                    else:\n",
    "                        translated_text = translator.translate(f\"Nationality for {person_name_upper} is not found!\", src='en', dest=selected_language).text\n",
    "                    if translated_text:\n",
    "                        get_chat_output(translated_text)\n",
    "            else:\n",
    "                get_entity(user_input)\n",
    "        \n",
    "    def get_chat_output(text):\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")  \n",
    "        time_label = tk.Label(chat_inner_frame, text=current_time, font=(\"Arial\", 8), fg=\"black\")\n",
    "        time_label.pack(anchor=\"n\", padx=5, pady=(10, 0))\n",
    "        button_bg = \"#00008B\"  \n",
    "        foreground = \"white\"\n",
    "        max_width = 30 \n",
    "        message=text\n",
    "        wrapped_message = textwrap.fill(message, width=max_width, replace_whitespace=False)\n",
    "\n",
    "        lines = wrapped_message.split('\\n')\n",
    "        label = tk.Label(chat_inner_frame, text=\"\\n\".join(lines), font=(\"Arial\", 10), bg=button_bg, fg=foreground, padx=5, pady=5, wraplength=200, justify=tk.LEFT, anchor='w')\n",
    "        label.pack(anchor=\"e\", padx=5, pady=5)\n",
    "        update_scrollregion()  # Update the scroll region\n",
    "        canvas.yview_moveto(1.0)  # Move the canvas down so that new messages are visible\n",
    "        canvas.update_idletasks()  \n",
    "        user_input.event_generate(\"<BackSpace>\")\n",
    "        chat_window.update()\n",
    "        chat_position_refresh()\n",
    "        \n",
    "    def get_current_weather(api_key,found_city,trazeni_datum):\n",
    "       # print(api_key,\"...\", found_city, \"...\", trazeni_datum)\n",
    "        url = f\"https://api.weatherapi.com/v1/current.json?key={api_key}&q={found_city}&dt={trazeni_datum}\"\n",
    "        response = requests.get(url)\n",
    "       # print(response)\n",
    "        if response.status_code==200:\n",
    "            weather_data=response.json()\n",
    "            current_weather=weather_data['current']['temp_c']\n",
    "            current_condition=weather_data['current']['condition']['text']\n",
    "            current_time = datetime.now()\n",
    "            current_hour=current_time.hour\n",
    "            current_minute=current_time.minute\n",
    "            if selected_language == \"en\":\n",
    "                message=f\"ChatBot: Current weather in {found_city.capitalize()}: Temperature: {current_weather}C, Condition: {current_condition}\\n\"\n",
    "                get_chat_output(message)   \n",
    "            else:\n",
    "                translated_city = translator.translate(found_city.capitalize(), src='en', dest=selected_language).text\n",
    "                message=f\"Current weather in {translated_city} at {current_hour}:{current_minute} is: Temperature: {current_weather}C, Condition: {current_condition}\"\n",
    "                translated_text = translator.translate(message, src='en', dest=selected_language).text\n",
    "                get_chat_output(translated_text)\n",
    "                \n",
    "    def check_weather(api_key,found_city,date,text):\n",
    "        rain= 1 if analyze_user_input(text,\"rain\") else 0\n",
    "        sun= 1 if analyze_user_input(text,\"sunny\") else 0 \n",
    "        wind= 1 if analyze_user_input(text,\"windy\") or analyze_user_input(text,\"wind\") else 0\n",
    "        today= 1 if analyze_user_input(text,\"today\") else 0\n",
    "        tomorrow= 1 if analyze_user_input(text,\"tomorrow\") else 0\n",
    "        \n",
    "        url = f\"https://api.weatherapi.com/v1/forecast.json?key={api_key}&q={found_city}&dt={date}&hourly=24\"\n",
    "        current_date = date.today()\n",
    "        set_hours=False\n",
    "        \n",
    "        if (date > current_date):\n",
    "            set_hours= True\n",
    "        response=requests.get(url)\n",
    "        message=\"\"\n",
    "        if response.status_code==200:\n",
    "            weather_data=response.json()\n",
    "            hourly_forecast = weather_data['forecast']['forecastday'][0]['hour']\n",
    "            current_time = datetime.now()\n",
    "            current_hour=current_time.hour\n",
    "            current_minute=current_time.minute\n",
    "            merged_forecast=[]\n",
    "            merged_forecast.append(f\"----  Forecast:{found_city.capitalize()}---Date:{date}----\\n\")\n",
    "            for hour_data in hourly_forecast:\n",
    "                time = hour_data['time']\n",
    "                time_obj = datetime.strptime(time, '%Y-%m-%d %H:%M')\n",
    "                hour = time_obj.hour\n",
    "                temperature = hour_data['temp_c']\n",
    "                condition = hour_data['condition']['text']\n",
    "                ## today-- \n",
    "                if set_hours==False:\n",
    "                    if hour > current_hour:\n",
    "                        if rain and today and \"rain\" in condition:\n",
    "\n",
    "                            message=(f\"Yes. Rain is expected in {found_city.capitalize()} at around {hour}:00 with temperature of: {temperature}C.\")\n",
    "                            if selected_language != \"english\":\n",
    "                                message=translator.translate(message, src='en', dest=selected_language).text\n",
    "                            rain=0\n",
    "                            get_chat_output(message)\n",
    "                            break\n",
    "                        elif sun and today and \"sunny\" in condition.lower():\n",
    "                            message=(f\"Yes. Sun is expected in {found_city.capitalize()} at around {hour}:00 with temperature of: {temperature}C.\")\n",
    "                            sun=0\n",
    "                            if selected_language != \"english\":\n",
    "                                message=translator.translate(message, src='en', dest=selected_language).text\n",
    "                            get_chat_output(message)\n",
    "                            break\n",
    "                        elif wind and today and \"wind\" in condition:\n",
    "                            message=(f\"Yes. Wind is expected in {found_city.capitalize()} at around {hour}:00 with temperature of: {temperature}C.\")\n",
    "                            wind=0\n",
    "                            if selected_language != \"english\":\n",
    "                                message=translator.translate(message, src='en', dest=selected_language).text\n",
    "                            get_chat_output(message)\n",
    "                            break\n",
    "                else:\n",
    "                        if rain and tomorrow and \"rain\" in condition:\n",
    "                            message=(f\"Yes. Rain is expected in {found_city.capitalize()} at around {hour}:00 with temperature of: {temperature}C.\")\n",
    "                            rain=0\n",
    "                            if selected_language != \"english\":\n",
    "                                message=translator.translate(message, src='en', dest=selected_language).text\n",
    "                            get_chat_output(message)\n",
    "                            break\n",
    "                        elif sun and tomorrow and \"sunny\" in condition.lower():\n",
    "                            message=(f\"Yes. Sun is expected in {found_city.capitalize()} at around {hour}:00 with temperature of: {temperature}C.\")\n",
    "                            sun=0\n",
    "                            if selected_language != \"english\":\n",
    "                                message=translator.translate(message, src='en', dest=selected_language).text\n",
    "                            get_chat_output(message)\n",
    "                            break\n",
    "                        elif wind and tomorrow and \"wind\" in condition:\n",
    "                            message=(f\"Yes. Wind is expected in {found_city.capitalize()} at around {hour}:00 with temperature of: {temperature}C.\")\n",
    "                            wind=0\n",
    "                            if selected_language != \"english\":\n",
    "                                message=translator.translate(message, src='en', dest=selected_language).text\n",
    "                            get_chat_output(message)\n",
    "                            break\n",
    "        if rain:\n",
    "            if today:\n",
    "                message=(f\"No. Rain is not expected in {found_city.capitalize()} today.\")\n",
    "            elif tomorrow:\n",
    "                message=(f\"No. Rain is not expected in {found_city.capitalize()} tomorrow.\")\n",
    "            if selected_language != \"english\":\n",
    "                message=translator.translate(message, src='en', dest=selected_language).text\n",
    "            get_chat_output(message)\n",
    "        if sun:\n",
    "            if today:\n",
    "                message=(f\"No. Sun is not expected in {found_city.capitalize()} today.\")\n",
    "            elif tomorrow:\n",
    "                message=(f\"No. Sun is not expected in {found_city.capitalize()} tomorrow.\")\n",
    "            if selected_language != \"english\":\n",
    "                message=translator.translate(message, src='en', dest=selected_language).text\n",
    "            get_chat_output(message)\n",
    "        if wind:\n",
    "            if today:\n",
    "                message=(f\"No. Wind is not expected in {found_city.capitalize()} today.\")\n",
    "            elif tomorrow:\n",
    "                message=(f\"No. Wind is not expected in {found_city.capitalize()} tomorrow.\")\n",
    "            if selected_language != \"english\":\n",
    "                message=translator.translate(message, src='en', dest=selected_language).text\n",
    "            get_chat_output(message)\n",
    "\n",
    "                \n",
    "    def get_weather_info(api_key,found_city, date):\n",
    "        url = f\"https://api.weatherapi.com/v1/forecast.json?key={api_key}&q={found_city}&dt={date}&hourly=24\"\n",
    "        set_hours=False\n",
    "        current_date = date.today()\n",
    "        if(date > current_date):\n",
    "            set_hours= True\n",
    "        response=requests.get(url)\n",
    "        if response.status_code==200:\n",
    "            weather_data=response.json()\n",
    "            hourly_forecast = weather_data['forecast']['forecastday'][0]['hour']\n",
    "            current_time = datetime.now()\n",
    "            current_hour=current_time.hour\n",
    "            current_minute=current_time.minute\n",
    "            merged_forecast=[]\n",
    "            merged_forecast.append(f\"----  Forecast:{found_city.capitalize()}---Date:{date}----\\n\")\n",
    "            \n",
    "            for hour_data in hourly_forecast:\n",
    "                time = hour_data['time']\n",
    "                time_obj = datetime.strptime(time, '%Y-%m-%d %H:%M')\n",
    "                hour = time_obj.hour\n",
    "                temperature = hour_data['temp_c']\n",
    "                condition = hour_data['condition']['text']\n",
    "                if set_hours==False:\n",
    "                    if hour > current_hour:\n",
    "\n",
    "                        if selected_language == \"en\":\n",
    "                            merged_forecast.append(message)\n",
    "                            message = f\"At {hour}:00, it is expected to be {current_condition} with a temperature of {current_weather}C in {found_city.capitalize()}.\\n\"\n",
    "                            merged_forecast.append(\"..............................\")\n",
    "                        else:\n",
    "                            translated_city = translator.translate(found_city.capitalize(), src='en', dest=selected_language).text\n",
    "                            translated_text = translator.translate(f\"At {hour}:00, it is expected to be {condition} with a temperature of {temperature}C in {found_city.capitalize()}.\\n\", src='en', dest=selected_language).text\n",
    "                            merged_forecast.append(translated_text)\n",
    "                            merged_forecast.append(\"..............................\")\n",
    "                else:\n",
    "                        if selected_language == \"en\":\n",
    "                            merged_forecast.append(message)\n",
    "                            message = f\"At {hour}:00, it is expected to be {current_condition} with a temperature of {current_weather}C in {found_city.capitalize()}.\\n\"\n",
    "                            merged_forecast.append(\"..............................\")\n",
    "                        else:\n",
    "                            translated_city = translator.translate(found_city.capitalize(), src='en', dest=selected_language).text\n",
    "                            translated_text = translator.translate(f\"At {hour}:00, it is expected to be {condition} with a temperature of {temperature}C in {found_city.capitalize()}.\\n\", src='en', dest=selected_language).text\n",
    "                            merged_forecast.append(translated_text)\n",
    "                            merged_forecast.append(\"..............................\")\n",
    "                        \n",
    "            if merged_forecast != \"\":\n",
    "               # print(merged_forecast)\n",
    "                get_chat_output(\"\\n\".join(merged_forecast))\n",
    "\n",
    "    def get_wiki_answer(text):\n",
    "        wiki_wiki = wikipediaapi.Wikipedia(\n",
    "                     language=\"en\",  \n",
    "                     extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "                     user_agent='YourAppName/1.0 (YourContactInfo)'\n",
    "                    )\n",
    "        page = wiki_wiki.page(text) \n",
    "        if page.exists():\n",
    "            print(\"Page exists!\")\n",
    "            print(\"Title:\", page.title)\n",
    "            #sentences = page.text.split('. ')\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', page.text)\n",
    "            first_two_sentences = sentences[:3]\n",
    "            combined_text = '\\n'.join(first_two_sentences)\n",
    "            if selected_language == \"english\" :\n",
    "                get_chat_output(combined_text)\n",
    "            else:\n",
    "                translated_text = translator.translate(combined_text,src=\"en\",dest=selected_language).text\n",
    "                get_chat_output(translated_text)\n",
    "                \n",
    "    def send_message1(event=None):\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")  \n",
    "        message = user_input.get(\"1.0\", tk.END).strip()  \n",
    "        print(message)\n",
    "        sender = \"User\"  \n",
    "        if message:\n",
    "            if sender == \"User\":\n",
    "                button_bg = \"#4CAF50\"  \n",
    "                foreground = \"white\"\n",
    "            else:\n",
    "                button_bg = \"blue\" \n",
    "                foreground = \"white\"\n",
    "\n",
    "         \n",
    "            time_label = tk.Label(chat_inner_frame, text=current_time, font=(\"Arial\", 8), fg=\"black\")\n",
    "            time_label.pack(anchor=\"n\", padx=5, pady=(10, 0))\n",
    "           # the message into multiple lines if it exceeds max_width\n",
    "            max_width = 30  # Adjust as needed\n",
    "            wrapped_message = textwrap.fill(message, width=max_width, replace_whitespace=False)\n",
    "            lines = wrapped_message.split('\\n')\n",
    "           # lines = [message[i:i+max_width] for i in range(0, len(message), max_width)]\n",
    "            #print(\"Width for wrapping:\", lines)\n",
    "            \n",
    "            # Create the message label\n",
    "            label = tk.Label(chat_inner_frame, text=\"\\n\".join(lines), font=(\"Arial\", 10), bg=button_bg, fg=foreground, padx=5, pady=5, wraplength=200, justify=tk.LEFT)\n",
    "           # label = tk.Label(chat_inner_frame, text=message, font=(\"Arial\", 11), bg=button_bg, fg=foreground, padx=5, pady=5, wraplength=300, justify=tk.LEFT)\n",
    "            label.pack(anchor=\"w\" if sender == \"User\" else \"e\", padx=5, pady=5)\n",
    "\n",
    "            if user_input.get(\"1.0\", \"end-1c\"):\n",
    "                user_input.delete(\"end-2c\", \"end-1c\")\n",
    "                user_input.mark_set(tk.INSERT, \"insert linestart\")\n",
    "            user_input.delete(\"1.0\", tk.END)\n",
    "            user_input.focus_set()\n",
    "            user_input.mark_set(tk.INSERT, \"0.0\")  \n",
    "            user_input.after(10, lambda: user_input.mark_set(tk.INSERT, \"1.0\"))\n",
    "            user_input.event_generate(\"<KeyPress-BackSpace>\")\n",
    "            chat_window.update()\n",
    "            update_scrollregion()\n",
    "            canvas.yview_moveto(1.0)\n",
    "        chat_window.update()\n",
    "        update_scrollregion()\n",
    "        detected_language = detect(message)\n",
    "        translated_input_en = translate_input(message,detected_language)\n",
    "        print(translated_input_en)\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(translated_input_en)\n",
    "        found_city = get_city(translated_input_en)\n",
    "        rest_of_forecast=0\n",
    "        kljucne_rijeci=[]\n",
    "        dan= 0\n",
    "        city_in_sentence=\"\"\n",
    "                ### if city not mentioned check the location data from ip address\n",
    "        if found_city == None:\n",
    "            city_in_sentence=None\n",
    "            location_data=get_location_data(get_ip_address())\n",
    "            if (location_data != None):\n",
    "                found_city=location_data['city']\n",
    "                print(\"User's current location:\",found_city)\n",
    "        current_keywords=[\"current\",\"currently\",\"right now\",\"now\"]\n",
    "             \n",
    "        current= 1 if any(analyze_user_input(translated_input_en,keyword) for keyword in current_keywords) else 0\n",
    "        news=1 if analyze_user_input(translated_input_en,\"news\") or analyze_user_input(translated_input_en,\"headlines\")  else 0\n",
    "        get_joke= 1 if analyze_user_input(translated_input_en,\"joke\") else 0\n",
    "        weather=1 if analyze_user_input(translated_input_en,\"weather\") else 0\n",
    "        \n",
    "        check_rain= 1 if analyze_user_input(translated_input_en,\"rain\") else 0\n",
    "        check_sunshine= 1 if analyze_user_input(translated_input_en,\"sunny\") else 0 ##sunshine sunny??'\n",
    "        check_wind= 1 if analyze_user_input(translated_input_en,\"windy\") or analyze_user_input(translated_input_en,\"wind\")  else 0\n",
    "        subject=check_if_subject(translated_input_en)\n",
    "        question= 1 if subject != None else 0\n",
    "        today= 1 if analyze_user_input(translated_input_en, \"today\") else 0\n",
    "        tomorrow= 1 if analyze_user_input(translated_input_en, \"tomorrow\") else 0\n",
    "        answered=False\n",
    "       # name_searched= check_if_person(translated_input_en)\n",
    "        names=[]\n",
    "        names=get_names(translated_input_en)\n",
    "        checked=False\n",
    "        \n",
    "        api_key = \"YOUR_API_KEY_HERE\"  # Replace \"YOUR_API_KEY_HERE\" with your  API key \n",
    "\n",
    "        found_response=None\n",
    "        for i in data['content']:\n",
    "            for pattern in i['patterns']:\n",
    "                if translated_input_en.lower() in pattern.lower():\n",
    "                    found_response = i['responses']\n",
    "                    break  # Exit the inner loop once a response is found\n",
    "                if found_response:\n",
    "                    break  # Exit the outer loop once a response is found\n",
    "        titles=[]\n",
    "        if translated_input_en:\n",
    "            if news==1 :\n",
    "                if analyze_user_input(translated_input_en,\"cnn\"):\n",
    "                    get_news_output(get_news_from_cnn())\n",
    "                elif analyze_user_input(translated_input_en,\"nyt\") :\n",
    "                    get_news_output(get_news_from_nyt())\n",
    "                elif analyze_user_input(translated_input_en,\"local\") or analyze_user_input(translated_input_en,\"news\") :\n",
    "                    country_code_from_ip=get_country_code(get_country_from_ip(get_ip_address()))\n",
    "                    if(country_code_from_ip.lower()=='ba'):\n",
    "                           get_news_output(get_news_from_klix()) \n",
    "                    elif (country_code_from_ip.lower()=='us'):\n",
    "                           get_news_output(get_news_from_nyt())\n",
    "            if get_joke:\n",
    "                message=get_random_joke(translated_input_en)\n",
    "                if selected_language != \"en\":\n",
    "                    message=translator.translate(message, src=\"en\", dest=selected_language).text\n",
    "                get_chat_output(message)\n",
    "            if names:\n",
    "                get_wiki_info(names, translated_input_en)\n",
    "                checked = True\n",
    "                answered=True\n",
    "            if question and weather and not found_response:\n",
    "                if not any([today, tomorrow, current]) and (not city_in_sentence or not found_city):\n",
    "                    get_wiki_answer(subject)\n",
    "                    answered=True\n",
    "                elif weather and not city_in_sentence and not any([current, today, tomorrow, names]):\n",
    "                    get_wiki_answer(subject)\n",
    "                    answered=True\n",
    "            elif question and not names and not any([names, check_rain, check_sunshine, check_wind, found_response]):\n",
    "                    get_wiki_answer(subject)\n",
    "                    answered=True\n",
    "            elif question and any([ check_rain, check_sunshine, check_wind]) and not any([names,weather,today,tomorrow]):\n",
    "                    get_wiki_answer(subject)\n",
    "                    answered=True\n",
    "            if any([check_rain, check_sunshine, check_wind]):\n",
    "                if today and not tomorrow:\n",
    "                    check_weather(api_key, found_city, date.today(), translated_input_en)\n",
    "                elif tomorrow and not today:\n",
    "                    check_weather(api_key, found_city, date.today() + timedelta(days=1), translated_input_en)\n",
    "\n",
    "        numbers_found=\"\"\n",
    "        pattern = r'\\b\\d{1,2}:\\d{2}\\b'\n",
    "        full_weather_forecast=0\n",
    "        number_in_sentence=0\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.text.isdigit():\n",
    "                number_in_sentence = int(token.text)\n",
    "            if \"by\" in token.text.lower():\n",
    "                full_weather_forecast=1 \n",
    "        #print(number_in_sentence)\n",
    "        #print(kljucne_rijeci)\n",
    "        \n",
    "        trazeni_datum=\"1/1/1111\"\n",
    "        if dan==0:\n",
    "            trazeni_datum = date.today()\n",
    "        elif dan==1:\n",
    "            trazeni_datum = date.today() + timedelta(days=1)\n",
    "           \n",
    "        print(trazeni_datum)\n",
    "            \n",
    "        \n",
    "        print(weather,current)\n",
    "        if (weather and found_city and today) or (weather and found_city and current) or (weather and city_in_sentence) or (check_rain and found_city) or (weather and tomorrow) or (weather and current):\n",
    "    \n",
    "            if (weather and current) or (weather and not today and not tomorrow):\n",
    "                get_current_weather(api_key,found_city,trazeni_datum)\n",
    "            elif (weather and today):\n",
    "                get_weather_info(api_key,found_city,trazeni_datum)\n",
    "            elif(weather and tomorrow):\n",
    "                trazeni_datum = date.today() + timedelta(days=1)\n",
    "                get_weather_info(api_key,found_city,trazeni_datum)\n",
    "        chat_position_refresh()\n",
    "        \n",
    "        \n",
    "        # Search for responses based on the user's input\n",
    "        if answered == False and not any ([weather,check_rain,check_sunshine, check_wind,news,get_joke]):\n",
    "            if found_response:\n",
    "                if selected_language == \"english\":\n",
    "                    get_chat_output(np.random.choice(found_response))\n",
    "                else:\n",
    "                    # Translate the response back to the user's selected language before displaying\n",
    "                    translated_response = [translator.translate(response, src='en', dest=selected_language).text for response in found_response]\n",
    "                    get_chat_output(np.random.choice(translated_response))\n",
    "            else:\n",
    "                result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([translated_input_en]), truncating='post', maxlen=max_len))\n",
    "                tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
    "                for i in data['content']:\n",
    "                    if i['tag'] == tag:\n",
    "                        if detected_language == selected_language:\n",
    "                            chat_log.insert(tk.END, \"ChatBot: \" + np.random.choice(i['responses']) + \"\\n\")\n",
    "                            get_chat_output(np.random.choice(i['responses']))\n",
    "                        else:\n",
    "                            translated_responses = [translator.translate(response, src='en', dest=selected_language).text for response in i['responses']]\n",
    "                            get_chat_output(np.random.choice(translated_responses))\n",
    "                            break  # Exit the loop once a response is found\n",
    "\n",
    "    def analyze_user_input(user_input,string):\n",
    "        doc = nlp(user_input)\n",
    "        for token in doc:\n",
    "                if token.text.lower() == string.lower():\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def check_if_subject(user_input):\n",
    "        print(\"User_input\",user_input)\n",
    "        doc = nlp(user_input)\n",
    "        subject = None\n",
    "        for token in doc:\n",
    "            if token.dep_ == \"nsubj\" and (token.pos_ == \"PROPN\" or token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\" or token.pos_ == \"PRON\"):\n",
    "                subject = token.text\n",
    "                print(\"Subject-..\",token.text)\n",
    "                break\n",
    "        return subject\n",
    "        \n",
    "    def get_names(user_input):\n",
    "        cleaned_name=\"\"\n",
    "        names=[]\n",
    "        doc = nlp(user_input)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                cleaned_name = clean_name(ent.text)\n",
    "                names.append(ent.text)\n",
    "        return names\n",
    "    \n",
    "    def clean_name(text):\n",
    "        doc = nlp(text)\n",
    "        clean_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'VERB':\n",
    "                clean_text.append(token.text)\n",
    "        return ' '.join(clean_text)\n",
    "    \n",
    "\n",
    "    def on_mouse_wheel(event):\n",
    "        canvas.yview_scroll(int(-1*(event.delta/120)), \"units\")\n",
    "\n",
    "    def remove_placeholder(event):\n",
    "        if user_input.get(\"1.0\", tk.END).strip() == \"Write your message here...\":\n",
    "            user_input.delete(\"1.0\", tk.END)\n",
    "    def on_enter(event):\n",
    "        send_message1()\n",
    "    def ignore_enter(event):\n",
    "        return \"break\"\n",
    "    def update_scrollregion(event=None):\n",
    "        canvas.itemconfig(chat_inner_frame_window, width=canvas.winfo_width())\n",
    "        canvas.configure(scrollregion=canvas.bbox(\"all\"))\n",
    "    \n",
    "    chat_window = tk.Toplevel(root)\n",
    "    chat_window.title(\"Chat Window\")\n",
    "    chat_window.geometry(\"400x400\")  \n",
    "\n",
    "    #chat_frame = tk.Frame(chat_window)\n",
    "    chat_frame = tk.Frame(chat_window, height=400, width=400)\n",
    "\n",
    "    chat_frame.pack(fill=tk.BOTH, expand=True, anchor=\"n\")\n",
    "\n",
    "    canvas = tk.Canvas(chat_frame)\n",
    "    canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "    scrollbar = tk.Scrollbar(chat_frame, orient=tk.VERTICAL, command=canvas.yview)\n",
    "    scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "    canvas.config(yscrollcommand=scrollbar.set)\n",
    "\n",
    "    chat_inner_frame = tk.Frame(canvas)\n",
    "    canvas.create_window((0, 0), window=chat_inner_frame, anchor=\"nw\")\n",
    "\n",
    "    canvas.bind('<Configure>', update_scrollregion)\n",
    "\n",
    "    chat_inner_frame_window = canvas.create_window((0, 0), window=chat_inner_frame, anchor=\"nw\")\n",
    "\n",
    "    canvas.bind_all(\"<MouseWheel>\", on_mouse_wheel)\n",
    "\n",
    "    user_input = tk.Text(chat_window, wrap=tk.WORD, width=40, height=2)\n",
    "    user_input.pack(side=tk.BOTTOM, fill=tk.X, padx=5, pady=5)\n",
    "    user_input.insert(\"1.0\", \"Write your message here...\")\n",
    "    user_input.bind(\"<FocusIn>\", remove_placeholder)\n",
    "    user_input.bind(\"<Return>\", send_message1)\n",
    "    \n",
    "    user_input.after(10, lambda: user_input.mark_set(tk.INSERT, \"1.0\"))\n",
    "    canvas.update_idletasks()  \n",
    "    user_input.focus_set()\n",
    "    user_input.mark_set(tk.INSERT, \"1.0\")  \n",
    "    user_input.after(10, lambda: user_input.mark_set(tk.INSERT, \"1.0\"))\n",
    "    canvas.update_idletasks()  \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "root = tk.Tk()\n",
    "root.geometry('400x200')\n",
    "root.title(\"Main Window\")\n",
    "\n",
    "ttk.Label(root, text=\"Select a Language:\",font=(\"Ariel\", 10)).grid(column=0, row=15, padx=10, pady=25)\n",
    "\n",
    "with open('languages.json', 'r') as lang_file:\n",
    "    language_data = json.load(lang_file)\n",
    "languages = [lang['name'] for lang in language_data['languages']]\n",
    "\n",
    "with open('chatbot_data.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "selected_language = tk.StringVar()\n",
    "\n",
    "languagechoosen = ttk.Combobox(root, width=27, textvariable=selected_language)\n",
    "\n",
    "languagechoosen['values'] = languages\n",
    "\n",
    "languagechoosen.grid(column=1, row=15)\n",
    "\n",
    "languagechoosen.current(21)\n",
    "\n",
    "open_chat_button = ttk.Button(root, text=\"Open Chat\", command=lambda: open_chat_window(selected_language.get()))\n",
    "open_chat_button.grid(column=1, row=16)\n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48353e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
